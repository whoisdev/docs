---
title: "Quickstart"
description: "Make your first Notebooks API call in under 2 minutes."
---

<Info>
  **Prerequisites:**
  - A [Notebooks account](https://dashboard.notebooks.app) with an active subscription
  - An API key (you'll create one below)
  - A notebook with at least one node (you'll need its `nodeId`)
</Info>

## Step 1: Create an API key

1. Go to [Settings > API](https://dashboard.notebooks.app/dashboard/settings/api) in your dashboard.
2. Click **Create API Key**.
3. Copy the key. It starts with `nb_sk_` and you'll only see it once.

<Warning>
  Store your key somewhere safe. You won't be able to see it again after you leave the page.
</Warning>

## Step 2: Find your node ID

Every API request needs a `nodeId` — this tells the AI which notebook node to use as context.

1. Open a notebook in your dashboard.
2. Click on a node.
3. Copy the node ID from the URL or node settings.

## Step 3: Install an SDK

The Notebooks API is OpenAI-compatible, so you can use the OpenAI SDK or the Vercel AI SDK.

<CodeGroup>

```bash OpenAI SDK (Python)
pip install openai
```

```bash OpenAI SDK (Node.js)
npm install openai
```

```bash Vercel AI SDK
npm install ai @ai-sdk/openai
```

</CodeGroup>

## Step 4: Make your first request

Replace `YOUR_API_KEY` and `YOUR_NODE_ID` below and run the code:

<CodeGroup>

```python OpenAI SDK (Python)
from openai import OpenAI

client = OpenAI(
    api_key="YOUR_API_KEY",
    base_url="https://dashboard.notebooks.app/api/v1",
)

response = client.chat.completions.create(
    model="anthropic/claude-sonnet-4.6",
    messages=[
        {"role": "user", "content": "Summarize the key topics in this notebook."}
    ],
    extra_body={
        "nodeId": "YOUR_NODE_ID",
    },
)

print(response.choices[0].message.content)
```

```typescript OpenAI SDK (Node.js)
import OpenAI from "openai";

const client = new OpenAI({
  apiKey: "YOUR_API_KEY",
  baseURL: "https://dashboard.notebooks.app/api/v1",
});

const response = await client.chat.completions.create(
  {
    model: "anthropic/claude-sonnet-4.6",
    messages: [
      { role: "user", content: "Summarize the key topics in this notebook." },
    ],
  },
  {
    body: {
      nodeId: "YOUR_NODE_ID",
    },
  }
);

console.log(response.choices[0].message.content);
```

```typescript Vercel AI SDK
import { createOpenAI } from "@ai-sdk/openai";
import { generateText } from "ai";

const notebooks = createOpenAI({
  apiKey: "YOUR_API_KEY",
  baseURL: "https://dashboard.notebooks.app/api/v1",
});

const { text } = await generateText({
  model: notebooks("anthropic/claude-sonnet-4.6"),
  prompt: "Summarize the key topics in this notebook.",
  providerOptions: {
    openai: {
      nodeId: "YOUR_NODE_ID",
    },
  },
});

console.log(text);
```

```bash curl
curl -X POST https://dashboard.notebooks.app/api/v1/chat \
  -H "Authorization: Bearer YOUR_API_KEY" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "anthropic/claude-sonnet-4.6",
    "nodeId": "YOUR_NODE_ID",
    "messages": [
      {
        "role": "user",
        "content": "Summarize the key topics in this notebook."
      }
    ]
  }'
```

</CodeGroup>

## Step 5: See the response

You'll get back a standard OpenAI `ChatCompletion` response:

```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "model": "anthropic/claude-sonnet-4.6",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Based on your notebook, here are the key topics..."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 1250,
    "completion_tokens": 340,
    "total_tokens": 1590
  }
}
```

If you've used the OpenAI API before, this will feel familiar — the response format is identical.

## What's next?

<Columns cols={3}>
  <Card title="Authentication" icon="key" href="/authentication">
    Secure your API keys properly.
  </Card>
  <Card title="Models" icon="microchip" href="/models">
    Pick the right model for your use case.
  </Card>
  <Card title="Streaming" icon="bolt" href="/streaming">
    Get real-time responses with SSE streaming.
  </Card>
</Columns>
